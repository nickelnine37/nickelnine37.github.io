<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta charset="utf-8" content="text/html" http-equiv="Content-Type">
<meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible"/>
<title>Building a Neural Net from Scratch with NumPy</title>
<meta content="True" name="HandheldFriendly"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<meta content="origin" name="referrer"/>
<meta content="Pelican" name="generator"/>
<link href="" rel="canonical"/>
<!-- Feed -->
<link href="/theme/css/style.css" rel="stylesheet" type="text/css"/>
<!-- Code highlight color scheme -->
<link href="/theme/css/code_blocks/c3.css" rel="stylesheet"/>
<!-- CSS specified by the user -->
<link href="//home/ed/Documents/Code/PA2/attila/static/css/my_colors.css" rel="stylesheet" type="text/css">
<!-- Custom fonts -->
<link href="https://fonts.googleapis.com/css?family=Montserrat:400,300" rel="stylesheet" type="text/css"/>
<link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css"/>
<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
  <![endif]-->
<link href="/NumPy-Neural-Net.html" rel="canonical"/>
<meta content="How to use NumPy library to build a neural net classifier for handwritten digits in Python" name="description"/>
<meta content="Edward Antonian" name="author"/>
<meta content="data science" name="tags"/>
<meta content="neural networks" name="tags"/>
<meta content="machine learning" name="tags"/>
<meta content="ai" name="tags"/>
<meta content="python" name="tags"/>
<meta content="NumPy" name="tags"/>
<meta content="MNIST" name="tags"/>
<meta content="backpropagation" name="tags"/>
<meta content="" property="og:locale">
<meta content="But is it Data Science?" property="og:site_name">
<meta content="website" property="og:type">
<meta content="But is it Data Science?" property="og:title">
<meta content="View the blog." property="og:description">
<meta content="" property="og:url">
<meta content="/static/images/contact-bg.jpg" property="og:image">
<meta content="https://facebook.com/myprofile" property="article:publisher">
<meta content="article" property="og:type"/>
<meta content="/author/edward-antonian.html" property="article:author"/>
<meta content="/NumPy-Neural-Net.html" property="og:url"/>
<meta content="Building a Neural Net from Scratch with NumPy" property="og:title"/>
<meta content="2018-03-03 00:00:00+00:00" property="article:published_time"/>
<meta content="How to use NumPy library to build a neural net classifier for handwritten digits in Python" property="og:description"/>
<meta content="/static/images/contact-bg.jpg" property="og:image"/>
<meta content="summary_large_image" name="twitter:card"/>
<meta content="@myprofile" name="twitter:site"/>
<meta content="Building a Neural Net from Scratch with NumPy" name="twitter:title"/>
<meta content="/NumPy-Neural-Net.html" name="twitter:url"/>
<meta content="/static/images/contact-bg.jpg" name="twitter:image:src"/>
<meta content="How to use NumPy library to build a neural net classifier for handwritten digits in Python" name="twitter:description"/>
</meta></meta></meta></meta></meta></meta></meta></meta></link></meta></head>
<!-- TODO : Body class -->
<body class="home-template">
<nav id="menu">
<a class="close-button">Close</a>
<div class="nav-wrapper">
<p class="nav-label">Menu</p>
<ul><li role="presentation"><a href="pages/about.html">About</a></li><li role="presentation"><a href="archives.html">Archive</a></li>
</ul>
</div>
</nav>
<!-- Progressbar -->
<div class="progress-container">
<span class="progress-bar"></span>
</div>
<!-- Page Header -->
<!-- Set your background image for this header on the line below. -->
<header class="has-cover" id="post-header">
<div class="inner">
<nav id="navigation">
<span class="nav-button" id="home-button">
<a class="home-button" href="index.html" title="Home"><i class="ic ic-arrow-left"></i> Home</a>
</span>
<span class="nav-button" id="menu-button">
<a class="menu-button"><i class="ic ic-menu"></i> Menu</a>
</span>
</nav>
<h1 class="post-title">Building a Neural Net from Scratch with NumPy</h1>
<!-- TODO : Proper class for headline -->
<span class="post-meta">
<a href="/author/edward-antonian.html">Edward antonian</a>
            | <time datetime="Sat 03 March 2018">Sat 03 March 2018</time>
</span>
<!-- TODO : Modified check -->
<span class="post-meta"> | Updated on Fri 25 May 2018</span>
<div class="post-cover cover" style="background-image: url('static/images/contact-bg.jpg')">
</div>
</div></header>
<section id="wrapper">
<a class="hidden-close"></a>
<!-- Post content -->
<main class="content" role="main">
<article class="post">
<div class="inner">
<section class="post-content">
<h1>Building a Simple Neural Network from Scratch with NumPy</h1>
<h2>SkyNet0.1 is here...</h2>
<p>Greetings and welcome to another blog post. It has been a while since my first little series and this time I want to try something a bit different. First though I just want to says thanks to everyone who read the last post - I really appreciate it. There's still plenty of stuff I can improve on so if you have any suggestions for how I can make the code better, or even ideas for new projects I'd love it if you dropped me a message. Hopefully I'll get a comments section up and running soon too.   </p>
<p>Anyway....</p>
<p>We all know the impending AI revolution is about to wipe us all off the face of the earth. And that seems to be a step closer now that I have built an algorithm capable of TELLING WITHIN 95% ACCURACY WHAT DIGIT FROM 0-9 IS WRITTEN ON A 28x28 PIXEL PICTURE!! It's chilling stuff. </p>
<p>Hilarious nerd banter aside, there is no shortage of blog posts about building neural nets, nor about using the MNIST data set. However I wanted to make a quick-and-easy guide to building a very simple neural net without using a tonne of fancy packages. The only real feature I'm using on top of basic Python is NumPy's vector and matrix multiplication functions, as python loops would be tragically slow. It's also a really nice way of making sure you understand the underlying maths, as opposed to plugging your problem into someone else's model. To follow this you'll need intermediate python, including OOP, and a bit of vector calculus (and maybe too much time on your hands?).</p>
<p>I will briefly summarise the problem I am trying to solve. The MNIST data set is a selection of thousands of handwritten digits from 0-9 fitted onto a 28 x 28 pixel array. Each pixel has a value between 0 and 256, with 256 indicating fully-filled and 0 representing empty. From these low-fi images we want to create an algorithm able to correctly identify which digit has been written. For a human this is of course easy. But how we might translate our human intuition into machine language is not immediately obvious. Machine learning offers a solution. By feeding our algorithm a huge data set of examples, it can begin to learn the patterns behind our intuition.  </p>
<p>Some example digits are below. Our task would therefore be for the computer to correctly interpret these as 5, 0, 4, 1</p>
<p><img alt="mnist_digits.png" src="images/mnist_digits.png"/></p>
<p>Lets start with a quick round-up of the basics of neural networks. If you want a much better introduction I highly recommend the video series from 3Blue1Brown which you can find <a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi">here</a>. For a more mathematical, in depth approach try <a href="http://neuralnetworksanddeeplearning.com/chap1.html">Michael Nielsen's posts</a>. </p>
<p>Neural nets are essentially a giant function. They take inputs and produce some output. In the simplest form, the inputs and outputs are just an array of numbers that represent some data. The 'neural' part arises from the function's internal structure which is supposed to loosely mimic the way our brain passes electrical signals between neurons. The key idea is to structure the network in layers of 'neurons'. The best number of neurons and layers to use is generally not obvious and can be tweaked to improve results. </p>
<p>In this case, our input neurons are the 28 x 28 = 784 individual pixel values from our black and white image. We can represent this as a vector called <span class="math">\(A^{0}\)</span> which has 784 components <span class="math">\(A_{j}^{0}\)</span> (superscripts are not powers in this case). The second and third layers will also be represented as a vector of numbers and they will be called <span class="math">\(A^{1}\)</span> and <span class="math">\(A^{2}\)</span>. In our case they will both have 16 components. Finally we will have an output layer <span class="math">\(A^{3}\)</span> which will have 10 components representing each one of our 10 possible digits from 0-9. </p>
<p>Each neuron (or component of the vector) will be a weighted sum of every neuron value from the previous level plus a constant term called the bias. This is then passed into a sigmoid function <span class="math">\(\sigma(x)\)</span> that squashes the value between 0 and 1. This function is defined as</p>
<div class="math">$$\sigma(x) = \frac{1}{1 + e^{-x}}$$</div>
<p>Thus, the first component of the second layer is defined as </p>
<div class="math">$$A^{1}_{0} = \sigma \; ( \; \sum_{j=0}^{783} w_{j} A^{0}_{j} + b \; ) $$</div>
<p>where <span class="math">\(w_{j}\)</span> is the weight attaching the <span class="math">\(j\)</span>th input neuron to the first second-layer neuron. In fact, we can write the <span class="math">\(k\)</span>th component of the second layer as</p>
<div class="math">$$A^{1}_{k} = \sigma \; ( \; \sum_{j} w^{1}_{kj} A^{0}_{j} + b^{1}_{k} \; ) $$</div>
<p>To those familiar with linear algebra, this is essentially a matrix multiplication operation. We could write this in a more compact way as</p>
<div class="math">$$ A^{1} = \sigma \; ( \; w^{1} A^{0} + b^{1} \; ) $$</div>
<p>where <span class="math">\(w^{1}\)</span> is a 784 x 16 matrix, <span class="math">\(b^{1}\)</span> is a 16-component vector, and it is understood that the sigmoid function is being applied element-wise. This pattern continues throughout the layers such that </p>
<div class="math">$$ A^{2} = \sigma \; ( \; w^{2} A^{1} + b^{2} \; ), \quad \text{and} \quad A^{3} = \sigma \; ( \; w^{3} A^{2} + b^{3} \; ) $$</div>
<p>This gives (784 x 16 + 16) + (16 x 16 + 16) + (16 x 10 + 10) = 13,002 possible variables. We can now edit each one individually in the hope that we come up with an arrangement that produces the desired output for any input. For example, if the input represents a zero, we hope our final layer <span class="math">\(A^{3}\)</span> is [1,0,0,0,0,0,0,0,0,0]. </p>
<p>Now assuming that's even possible, how do we go about adjusting the weights? The first thing is to construct a cost function. In this case we will use half the square of the euclidean distance between the target output and the produced output, i.e.</p>
<div class="math">$$ C = \frac{1}{2} \sum ( y_{i} - A^{3}_{i} ) ^{2} $$</div>
<p>where <span class="math">\(y_i\)</span> is the correct output. We now want to find a global minimum to this cost function space that exists in 13,002 dimensions. Vector calculus tells us that the direction of steepest descent is given by the vector </p>
<div class="math">$$ - \frac{\partial C}{\partial x_{i} } $$</div>
<p>for each possible variable <span class="math">\(x_{i}\)</span>. So we should iterate over and over, replacing <span class="math">\(x_{i} \rightarrow x_{i} -\eta \frac{\partial C}{\partial x_{i}} $, where $\eta\)</span> is a variable called the learning rate. For complicated functions, often the best hope of finding these derivatives is to evaluate the function at two close points like so</p>
<div class="math">$$ \frac{\partial C}{\partial x_{i}} \approx \frac{1}{\Delta x_{i} } \big(C(x_{i} + \Delta x_{i}) - C({x_{i}})\big) $$</div>
<p>However for 13,002 variables this would be hopelessly slow. Luckily, we can algebraically determine the gradient of the cost function in each direction in a process called back-propagation. This saves us making thousand of function calls and has the benefit of perfect precision up to your floating point limit. </p>
<p>This is where the maths starts to get a little more involved. This is why people generally use tools like TensorFlow to build their neural networks - the algebra is all handled behind the scene. Back propagation essentially uses the chain rule to determine how the final layer variables respond to small changes in weights and biases further down to network. I wont try and go through the maths in great detail, I will just show the result. Michael Nielsen's posts do a very good job if you're interested. </p>
<p>Define:
</p>
<div class="math">$$ z^{l} = w^{l} A^{l-1} + b^{l} $$</div>
<p>
such that 
</p>
<div class="math">$$ a^{l} = \sigma(z^{l}) $$</div>
<p>
Define 
</p>
<div class="math">$$\delta^3_j = \frac{\partial C}{\partial A^3_j} \sigma'(z^3_j).$$</div>
<p>
and note that
</p>
<div class="math">$$\frac{\partial C}{\partial A^3_j} = (A_j^3-y_j)$$</div>
<p>
Thus
</p>
<div class="math">$$\delta^{3} = (A^3-y) \odot \sigma'(z^3) $$</div>
<p>where the <span class="math">\(\odot\)</span> symbol means the two vectors are multiplied element wise. we then have for <span class="math">\(l&lt;3\)</span>:</p>
<div class="math">$$ \delta^l = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) $$</div>
<p>Finally we get </p>
<div class="math">$$\frac{\partial C}{\partial b^{l}_{j}} = \delta^{l}_j \quad \text{and} \quad \frac{\partial C}{\partial w^{l}_{jk}} = A_{k}^{l-1} \delta^{l}_j$$</div>
<p>And that's pretty much it! We now just need to average the changes to the weights and biases across every example in our training set and repeat, and watch the accuracy improve. </p>
<p>WELL. Thank Christ that's over. Lets get on to the fun bit. First we need to import the relevant modules. I know I said we'd only use NumPy. But we're also going to use the mnist package that lets you handle the data a bit easier. </p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">mnist</span> <span class="kn">import</span> <span class="n">MNIST</span>
</pre></div>
<p>Then we'll define the sigmoid and sigmoid prime functions</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">sigmoid_dash</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">s</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">s</span><span class="p">)</span>
</pre></div>
<p>Then we'll extract the data. Make sure you've downloaded it and saved it into your desired directory. </p>
<div class="highlight"><pre><span></span><span class="n">mndata</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s1">'/home/ed/Documents/Code/Projects/ML/MNIST'</span><span class="p">)</span>


<span class="n">training_data</span><span class="p">,</span> <span class="n">training_labels</span> <span class="o">=</span> <span class="n">mndata</span><span class="o">.</span><span class="n">load_training</span><span class="p">()</span>
<span class="n">test_data</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">mndata</span><span class="o">.</span><span class="n">load_testing</span><span class="p">()</span>

<span class="n">training_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">training_data</span><span class="p">]</span>
<span class="n">test_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">]</span>

<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">training_labels</span><span class="p">)</span>
</pre></div>
<p>Then define the layer structure</p>
<div class="highlight"><pre><span></span><span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="mi">784</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
<span class="n">n_layers</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
</pre></div>
<p>Let's initialise the weights and biases randomly</p>
<div class="highlight"><pre><span></span><span class="n">initial_weights</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
                   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

<span class="n">initial_biases</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
                  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</pre></div>
<p>ok. Now we'll start building our model class as class: Model. I'll take you through the methods one by one, then put them all together. The <strong>init</strong> method just takes the initial weights and biases and adds them to the class. If you're picking up from before, you can also load them from a previous run. </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">initial_biases</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">from_saved</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">from_saved</span><span class="p">:</span>
        <span class="n">saved_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'best_weights.npz'</span><span class="p">)</span>
        <span class="n">saved_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'best_biases.npz'</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">saved_w</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">saved_w</span><span class="o">.</span><span class="n">files</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="n">saved_b</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">saved_b</span><span class="o">.</span><span class="n">files</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">initial_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">initial_biases</span>
</pre></div>
<p>The propogate method just takes the input neurons and runs them through the network</p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">propagate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_neurons</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_neurons</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>
<p>The answer method just gives us a neat way to visualise the answer given </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">answer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_neurons</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">input_neurons</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
<p>The learn method goes through every example in the training data, works out the corresponding changes that should be made to the weights, and averages them to find a final proposed change. It takes the learning rate hyperparamter. </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>

    <span class="n">dw</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="k">for</span> <span class="n">W</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">]</span>
    <span class="n">db</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">B</span><span class="p">)</span> <span class="k">for</span> <span class="n">B</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">input_neurons</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">):</span>

        <span class="n">true_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">true_output</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">input_neurons</span><span class="p">)</span>

        <span class="n">d</span> <span class="o">=</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">true_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_dash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
            <span class="n">d</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_dash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)]))</span>

        <span class="n">d</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
        <span class="n">dCdb</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">dCdw</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">dw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dCdw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">db</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dCdb</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">dw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">db</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
<p>Define a simple train method to repeat learning. If the mode produces weights and biases with better accuracy we save them. </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_loops</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'Beginning Training...'</span><span class="p">)</span>
    <span class="n">best_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_loops</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
        <span class="n">this_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">this_score</span> <span class="o">&gt;=</span> <span class="n">best_score</span><span class="p">:</span>
            <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="s1">'best_weights.npz'</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
            <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="s1">'best_biases.npz'</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
            <span class="n">best_score</span> <span class="o">=</span> <span class="n">this_score</span>
</pre></div>
<p>Finally, a simple method to neaty evaluate the model. </p>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="n">incorrect</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">input_neurons</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">answer</span><span class="p">(</span><span class="n">input_neurons</span><span class="p">)</span> <span class="o">==</span> <span class="n">label</span><span class="p">:</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">incorrect</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">fraction</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="p">(</span><span class="n">correct</span> <span class="o">+</span> <span class="n">incorrect</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">'{:.2f}</span><span class="si">% c</span><span class="s1">orrect'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">fraction</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">fraction</span>
</pre></div>
<p>And that's it! All we need to do now is instantiate the class and run</p>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Model</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">initial_weights</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">initial_biases</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">from_saved</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">from_saved</span><span class="p">:</span>
            <span class="n">saved_w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'best_weights.npz'</span><span class="p">)</span>
            <span class="n">saved_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">'best_biases.npz'</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="n">saved_w</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">saved_w</span><span class="o">.</span><span class="n">files</span><span class="p">)]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="p">[</span><span class="n">saved_b</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">saved_b</span><span class="o">.</span><span class="n">files</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">initial_weights</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">initial_biases</span>

    <span class="k">def</span> <span class="nf">propagate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_neurons</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_neurons</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">answer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_neurons</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">input_neurons</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>

        <span class="n">dw</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span> <span class="k">for</span> <span class="n">W</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">]</span>
        <span class="n">db</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">B</span><span class="p">)</span> <span class="k">for</span> <span class="n">B</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">input_neurons</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">training_labels</span><span class="p">):</span>

            <span class="n">true_output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">true_output</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="n">input_neurons</span><span class="p">)</span>

            <span class="n">d</span> <span class="o">=</span> <span class="p">[(</span><span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">true_output</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigmoid_dash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
                <span class="n">d</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">sigmoid_dash</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)]))</span>

            <span class="n">d</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>
            <span class="n">dCdb</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="n">dCdw</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">layers</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">layers</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
                <span class="n">dw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dCdw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">db</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dCdb</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">dw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="p">(</span><span class="n">learning_rate</span> <span class="o">/</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">db</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">training_loops</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'Beginning Training...'</span><span class="p">)</span>
        <span class="n">best_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">training_loops</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>
            <span class="n">this_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate_model</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">this_score</span> <span class="o">&gt;=</span> <span class="n">best_score</span><span class="p">:</span>
                <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="s1">'best_weights.npz'</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
                <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="s1">'best_biases.npz'</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
                <span class="n">best_score</span> <span class="o">=</span> <span class="n">this_score</span>

    <span class="k">def</span> <span class="nf">evaluate_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="n">incorrect</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">input_neurons</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">answer</span><span class="p">(</span><span class="n">input_neurons</span><span class="p">)</span> <span class="o">==</span> <span class="n">label</span><span class="p">:</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">incorrect</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">fraction</span> <span class="o">=</span> <span class="n">correct</span> <span class="o">/</span> <span class="p">(</span><span class="n">correct</span> <span class="o">+</span> <span class="n">incorrect</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s1">'{:.2f}</span><span class="si">% c</span><span class="s1">orrect'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">fraction</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">fraction</span>



<span class="n">my_model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">from_saved</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                 <span class="n">initial_weights</span><span class="o">=</span><span class="n">initial_weights</span><span class="p">,</span>
                 <span class="n">initial_biases</span><span class="o">=</span><span class="n">initial_biases</span><span class="p">)</span>
<span class="n">my_model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">training_loops</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
<div class="highlight"><pre><span></span><span class="n">Beginning</span> <span class="n">Training</span><span class="p">...</span>
<span class="mi">12</span><span class="p">.</span><span class="mi">47</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">10</span><span class="p">.</span><span class="mi">01</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">12</span><span class="p">.</span><span class="mi">01</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">12</span><span class="p">.</span><span class="mi">26</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">12</span><span class="p">.</span><span class="mi">51</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">12</span><span class="p">.</span><span class="mi">76</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">13</span><span class="p">.</span><span class="mi">95</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">15</span><span class="p">.</span><span class="mi">62</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">17</span><span class="p">.</span><span class="mi">12</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">18</span><span class="p">.</span><span class="mi">13</span><span class="o">%</span> <span class="n">correct</span>
<span class="mi">19</span><span class="p">.</span><span class="mi">26</span><span class="o">%</span> <span class="n">correct</span>
</pre></div>
<p>I managed to get my model up to 95% accuracy after about half an hour of training. Laughable for the MNIST data set? Maybe. But a lot of fun, and pretty cool anyway. </p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'red ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</section>
<section class="post-info">
<div class="post-share">
<a class="twitter" href="https://twitter.com/share?text=Building a Neural Net from Scratch with NumPy&amp;url=/NumPy-Neural-Net.html" onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
<i class="ic ic-twitter"></i><span class="hidden">Twitter</span>
</a>
<a class="facebook" href="https://www.facebook.com/sharer/sharer.php?u=/NumPy-Neural-Net.html" onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
<i class="ic ic-facebook"></i><span class="hidden">Facebook</span>
</a>
<a class="googleplus" href="https://plus.google.com/share?url=/NumPy-Neural-Net.html" onclick="window.open(this.href, 'google-plus-share', 'width=490,height=530');return false;">
<i class="ic ic-googleplus"></i><span class="hidden">Google+</span>
</a>
<div class="clear"></div>
</div>
<aside class="post-tags">
<a href="/tag/data-science.html">data science</a><a href="/tag/neural-networks.html">neural networks</a><a href="/tag/machine-learning.html">machine learning</a><a href="/tag/ai.html">ai</a><a href="/tag/python.html">python</a><a href="/tag/numpy.html">NumPy</a><a href="/tag/mnist.html">MNIST</a><a href="/tag/backpropagation.html">backpropagation</a> </aside>
<div class="clear"></div>
</section>
<aside class="post-nav">
<div class="clear"></div>
</aside>
</div>
</article>
</main>
<!-- TODO : Body class -->
<div class="" id="body-class" style="display: none;"></div>
<footer id="footer">
<div class="inner">
<section class="credits">
<span class="credits-theme">Theme <a href="https://github.com/arulrajnet/attila" rel="nofollow">Attila</a></span>
<span class="credits-software">Published with <a href="https://github.com/getpelican/pelican" rel="nofollow">Pelican</a></span>
</section>
</div>
</footer>
</section>
<script src="/theme/js/script.js" type="text/javascript"></script>
</body>
</html>